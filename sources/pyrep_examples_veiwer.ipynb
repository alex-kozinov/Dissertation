{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import dirname, join, abspath\n",
    "from pyrep import PyRep\n",
    "from pyrep.robots.arms.panda import Panda\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENE_FILE = \"examples/scene_panda_reach_target.ttt\"\n",
    "\n",
    "pr = PyRep()\n",
    "pr.launch(SCENE_FILE, headless=False)\n",
    "pr.start()\n",
    "agent = Panda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_joint_positions = agent.get_joint_positions()\n",
    "default_pos, default_quat = agent.get_tip().get_position(), agent.get_tip().get_quaternion()\n",
    "\n",
    "def move(new_pos):\n",
    "    new_joint_angles = agent.solve_ik_via_jacobian(new_pos, quaternion=quat)\n",
    "    agent.set_joint_target_positions(new_joint_angles)\n",
    "    pr.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.07810307, 0.02294977, 1.47322607])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    move(default_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# while True:\n",
    "time.sleep(5)\n",
    "MAX_DELTA = 0.01\n",
    "STEPS = 10\n",
    "\n",
    "for i in range(STEPS):\n",
    "    delta\n",
    "    move(default_pos - i*np.array([0, 0, DELTA]))\n",
    "\n",
    "for i in range(STEPS):\n",
    "#     pass\n",
    "#     move(default_pos)\n",
    "    move(default_pos - (STEPS - i - 1) *np.array([0, 0, DELTA]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pr.stop()\n",
    "pr.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panda reach target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached target 0!\n",
      "Reached target 1!\n",
      "Reached target 2!\n",
      "Reached target 3!\n",
      "Reached target 4!\n",
      "Reached target 5!\n",
      "Reached target 6!\n",
      "Reached target 7!\n",
      "Reached target 8!\n",
      "Reached target 9!\n",
      "CPU times: user 3.25 s, sys: 80.7 ms, total: 3.34 s\n",
      "Wall time: 5.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from os.path import dirname, join, abspath\n",
    "from pyrep import PyRep\n",
    "from pyrep.robots.arms.panda import Panda\n",
    "from pyrep.objects.shape import Shape\n",
    "from pyrep.const import PrimitiveShape\n",
    "from pyrep.errors import ConfigurationPathError\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "LOOPS = 10\n",
    "SCENE_FILE = 'examples/scene_panda_reach_target.ttt'\n",
    "pr = PyRep()\n",
    "pr.launch(SCENE_FILE, headless=True)\n",
    "pr.start()\n",
    "agent = Panda()\n",
    "\n",
    "# We could have made this target in the scene, but lets create one dynamically\n",
    "target = Shape.create(type=PrimitiveShape.SPHERE,\n",
    "                      size=[0.05, 0.05, 0.05],\n",
    "                      color=[1.0, 0.1, 0.1],\n",
    "                      static=True, respondable=False)\n",
    "\n",
    "position_min, position_max = [0.8, -0.2, 1.0], [1.0, 0.2, 1.4]\n",
    "\n",
    "starting_joint_positions = agent.get_joint_positions()\n",
    "\n",
    "for i in range(LOOPS):\n",
    "\n",
    "    # Reset the arm at the start of each 'episode'\n",
    "    agent.set_joint_positions(starting_joint_positions)\n",
    "\n",
    "    # Get a random position within a cuboid and set the target position\n",
    "    pos = list(np.random.uniform(position_min, position_max))\n",
    "    target.set_position(pos)\n",
    "\n",
    "    # Get a path to the target (rotate so z points down)\n",
    "    try:\n",
    "        path = agent.get_path(\n",
    "            position=pos, euler=[0, math.radians(180), 0])\n",
    "    except ConfigurationPathError as e:\n",
    "        print('Could not find path')\n",
    "        continue\n",
    "\n",
    "    # Step the simulation and advance the agent along the path\n",
    "    done = False\n",
    "    while not done:\n",
    "        done = path.step()\n",
    "        pr.step()\n",
    "\n",
    "    print('Reached target %d!' % i)\n",
    "\n",
    "pr.stop()\n",
    "pr.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 0\n",
      "Starting episode 1\n",
      "Starting episode 2\n",
      "Starting episode 3\n",
      "Starting episode 4\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from os.path import dirname, join, abspath\n",
    "from pyrep import PyRep\n",
    "from pyrep.robots.arms.panda import Panda\n",
    "from pyrep.objects.shape import Shape\n",
    "import numpy as np\n",
    "\n",
    "SCENE_FILE = 'examples/scene_reinforcement_learning_env.ttt'\n",
    "POS_MIN, POS_MAX = [0.8, -0.2, 1.0], [1.0, 0.2, 1.4]\n",
    "EPISODES = 5\n",
    "EPISODE_LENGTH = 200\n",
    "\n",
    "\n",
    "class ReacherEnv(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pr = PyRep()\n",
    "        self.pr.launch(SCENE_FILE, headless=False)\n",
    "        self.pr.start()\n",
    "        self.agent = Panda()\n",
    "        self.agent.set_control_loop_enabled(False)\n",
    "        self.agent.set_motor_locked_at_zero_velocity(True)\n",
    "        self.target = Shape('target')\n",
    "        self.agent_ee_tip = self.agent.get_tip()\n",
    "        self.initial_joint_positions = self.agent.get_joint_positions()\n",
    "\n",
    "    def _get_state(self):\n",
    "        # Return state containing arm joint angles/velocities & target position\n",
    "        return np.concatenate([self.agent.get_joint_positions(),\n",
    "                               self.agent.get_joint_velocities(),\n",
    "                               self.target.get_position()])\n",
    "\n",
    "    def reset(self):\n",
    "        # Get a random position within a cuboid and set the target position\n",
    "        pos = list(np.random.uniform(POS_MIN, POS_MAX))\n",
    "        self.target.set_position(pos)\n",
    "        self.agent.set_joint_positions(self.initial_joint_positions)\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.agent.set_joint_target_velocities(action)  # Execute action on arm\n",
    "        self.pr.step()  # Step the physics simulation\n",
    "        ax, ay, az = self.agent_ee_tip.get_position()\n",
    "        tx, ty, tz = self.target.get_position()\n",
    "        # Reward is negative distance to target\n",
    "        reward = -np.sqrt((ax - tx) ** 2 + (ay - ty) ** 2 + (az - tz) ** 2)\n",
    "        return reward, self._get_state()\n",
    "\n",
    "    def shutdown(self):\n",
    "        self.pr.stop()\n",
    "        self.pr.shutdown()\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "\n",
    "    def act(self, state):\n",
    "        del state\n",
    "        return list(np.random.uniform(-1.0, 1.0, size=(7,)))\n",
    "\n",
    "    def learn(self, replay_buffer):\n",
    "        del replay_buffer\n",
    "        pass\n",
    "\n",
    "\n",
    "env = ReacherEnv()\n",
    "agent = Agent()\n",
    "replay_buffer = []\n",
    "\n",
    "for e in range(EPISODES):\n",
    "\n",
    "    print('Starting episode %d' % e)\n",
    "    state = env.reset()\n",
    "    for i in range(EPISODE_LENGTH):\n",
    "        action = agent.act(state)\n",
    "        reward, next_state = env.step(action)\n",
    "        replay_buffer.append((state, action, reward, next_state))\n",
    "        state = next_state\n",
    "        agent.learn(replay_buffer)\n",
    "\n",
    "print('Done!')\n",
    "env.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
