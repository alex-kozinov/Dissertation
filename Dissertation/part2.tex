\chapter{Обзор литературы}\label{ch:ch2}
Среди последних работ, несколько разных подходов не только к обучению агента, но и к самому формату упровляющей программы. В статье \cite{kumar2018bipedal}  постановка задачи наиболее близка к постановке в текущей статье. Функция наград поощеряет продвижение робота вперёд, но штрафует за падения. Пространство состояний и действий имеют похожую структуру. Но у модели робота отсутсвуюут стопы и всего 6 степеней свободы, что является более простой моделью, по сравнению с роботом из текущей статьи. Обучение агента происходит при помощи Deep Deterministic Policy Gradient. В \cite{tidd2021guided} рассматривается последовательный метод обучения ходьбы по неравномерной поверхности. В качестве награды рассматривается взвешенная сумма наград за достижения целевой скорости, нарушение предельного крутящегося момента сервомоторов и попадание в целевую длину шага. Состояние расширено и содержит также ускорения всех сервомоторов, а также информацию о глубине наблюдаемого изображения. В качестве обучающего алгоритма используется Proximal Policy Optimisation