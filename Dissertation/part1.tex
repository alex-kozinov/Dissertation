\chapter{Постановка задачи обучения робота прямохождению}\label{ch:ch1}
\section{Обзор робота}\label{sec:ch1/sec1}
В данной задаче используется робот, собранный из сервомоторов Kondo,  неподвижных частей, с оборудованием для ориентации робота в пространстве: акселерометр, гироскоп, магнитометр (рисунок ~\cref{fig:real_robot}). Каждый сервомотор обеспечивает поворот вдоль заданной оси в заданном интервале углов. Движение может происходить как по часовой стрелке, так и против. Для каждого мотора задаётся свой интервал углов, чтобы избегать повреждения робота вследствие самопересечения конструкции робота. В итоге 23 сервомотора задают 23 степени свободы движений робота. Так как мы исследуем возможности ходьбы, то ограничимся только двенадцатью сервомоторами – по 6 для каждой ноги (рисунок ~\cref{fig:sim_robot_2-1})

В качестве источника входных параметров для модели используется инерциальный измерительный модуль IMU BNO055. Этот модуль позволяет измерять углы отклонения от основных осей внешней среды. При его помощи, например, можно определить в каком положении находится робот: стоит прямо на двух ногах или же упал и лежит горизонтально.

\begin{figure}[ht]
    \centerfloat{
        \label{fig:real_robot-1}}{%
            \includegraphics[width=0.5\linewidth]{real_robot}}
    \caption[Изображение робота в действии]{Изображение робота в действии}\label{fig:real_robot}
\end{figure}
\section{Обзор симулятора CoppeliaSim}\label{sec:ch1/sec1}

Для ускорения исследования используется симулятор CoppeliaSim \cite{coppeliaSim}. Работа в симуляторе позволяет производить параллельные вычисления, а также модифицировать параметры окружающей среды: изменять рельеф поверхности, добавлять препятствия и создавать толкающие усилия в направлении робота. 

В симуляторе представлена (рисунок ~\cref{fig:sim_robot_2-2}) упрощённая модель робота. Модель состоит из примитивных объектов: сфера, параллелепипед и цилиндрический сервомотор. Это позволяет значительно ускорить вычисления, при этом наследуя все важные для ходьбы физические свойства реальной модели.

\begin{figure}[ht]
    \centerfloat{
        \hfill
        \subcaptionbox[List-of-Figures entry]{Модель робота целиком\label{fig:sim_robot_2-1}}{%
            \includegraphics[width=0.4\linewidth]{sim_robot}}
        \hfill
        \subcaptionbox{Сервомоторы ноги \label{fig:sim_robot_2-2}}{%
            \includegraphics[width=0.328\linewidth]{foot}}
        \hfill
    }
    \caption[Обзор модели робота в~симуляции]{Обзор модели робота в~симуляции}\label{fig:sim_robot_2}
\end{figure}

Взаимодействие с симулятором будет осуществляться через проект PyRep \cite{james2019pyrep}. Преимущество этого подхода в том, что нет затрат времени на межпроцессорное взаимодействие программы обучения и симулятора, есть возможность запускать несколько симуляций параллельно. А что самое важное – есть возможность исполнения алгоритм без визуализации процесса. Такой подход помогает ускорять процедуру обучения, что позволяет проводить больше экспериментов.

\section{Определение марковского процесса принятия решений}\label{sec:ch1/sec3}
Одним из подходов описания процесса взаимодействия управляющей программы со средой является марковский процесс принятия решения. Процесс задаётся четвёркой $\left<S, A, T, R\right>$, где $S$~--- множество состояний среды, $A$~--- множество возможных действий управляющей программы. $T: S\times A \times S \rightarrow [0, +\inf)$ определяет распределение вероятности на множестве $S$, иначе говоря $\forall s, s' \in S,\ a\in A:\ T(s, a, s') = P(s' | s, a)$. Функция $R : S \times A \ \times S \rightarrow \mathbb{R}$ определяет награду, которую получает агент (управляющая программа), например, если, выполнив действие $a$, был осуществлён переход и состояния $s$ в состояние $s'$, то агент получается за этот ход награду, равную $R(s, a, s')$. Выбрать подходящую функцию награды~---  трудная задача. Обычно она решается в ходе экспериментов методом проб и ошибок 


Цель агента~--- максимизация дисконтированной суммы наград:

$$
\sum\limits_{i}^{i < k} \gamma ^{k - 1 - i} R(s_{i \cdot \tau}, a_{i \cdot \tau}, s_{(i+1) \cdot \tau})
$$

Где $\gamma \in [0, 1]$~--- параметр дисконтирования, а $k$~--- количество циклов с начала симуляции. 


Для возможности выполнения алгоритма непрерывное время в среде дискретизируется. А именно каждые $\tau$ миллисекунд состояние $s_t \in S$ меняется на $s_{t+\tau} \in S$ после выполнения действия $a \in A$.

В действительности вероятности переходов $T$ не являются известными, но можно описать множество событий и множество действий, а также награду и цель агента. Эти множества будут определяться в ходе экспериментов.

\section{Постановка цели работы}\label{sec:ch1/sec4}
Целью работы является определение множества состояний $S$, множества действий агента $A$ и подбор функции награды $R$, значения которой возвращает среда в ответ на каждое действие $a \in A$ агента. Эти параметры должны быть подобраны таким образом, чтобы в результате агент мог осуществлять реалистичную походку.

Более точнее критерий успеха можно определить следующим образом: ходьба осуществляется на двух ногах, робот двигается устойчиво в течении хотя бы 3-х секунд, перемещаясь вперёд относительно начального положения